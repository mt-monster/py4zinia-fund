# -*- coding: utf-8 -*-
"""
è¯åˆ¸è¡Œä¸šå®¢æˆ·æµå¤±é¢„è­¦å®Œæ•´æ¡ˆä¾‹
è¿è¡Œç¯å¢ƒ: Python 3.8+, ä¾èµ–åŒ…è§æ–‡æœ« requirements.txt
"""

import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, roc_auc_score
import xgboost as xgb
import shap
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

# ==================== 1. è¯åˆ¸è¡Œä¸šæ•°æ®æ¨¡æ‹Ÿ ====================
def generate_securities_data(n_customers=3000, n_weeks=12):
    """
    æ¨¡æ‹Ÿè¯åˆ¸å®¢æˆ·å‘¨åº¦æ•°æ®
    åŒ…å«èµ„äº§ã€äº¤æ˜“ã€è¡Œä¸ºç­‰è¯åˆ¸è¡Œä¸šæ ¸å¿ƒç‰¹å¾
    """
    np.random.seed(42)
    
    # å®¢æˆ·åŸºç¡€ä¿¡æ¯
    customers = []
    for i in range(1, n_customers + 1):
        customer_id = f"CUST_{i:06d}"
        
        # æ ¹æ®å®¢æˆ·ç±»å‹è®¾ç½®åŸºç¡€å‚æ•°
        customer_type = np.random.choice(['retail', 'vip', 'institution'], p=[0.7, 0.25, 0.05])
        base_assets = {
            'retail': np.random.uniform(5, 100),
            'vip': np.random.uniform(100, 1000),
            'institution': np.random.uniform(500, 5000)
        }[customer_type]
        
        customers.append({
            'customer_id': customer_id,
            'customer_type': customer_type,
            'age': np.random.randint(25, 70),
            'risk_tolerance': np.random.choice(['conservative', 'moderate', 'aggressive'], 
                                               p=[0.3, 0.5, 0.2]),
            'has_financial_product': np.random.choice([0, 1], p=[0.7, 0.3]),
            'has_visited': np.random.choice([0, 1], p=[0.6, 0.4]),  # æ˜¯å¦å½“é¢æ‹œè®¿
            'base_assets': base_assets
        })
    
    customer_df = pd.DataFrame(customers)
    
    # ç”Ÿæˆå‘¨åº¦æ•°æ®
    weekly_records = []
    start_date = datetime(2023, 1, 1)
    
    for _, customer in customer_df.iterrows():
        # ä¸ºæµå¤±å®¢æˆ·è®¾ç½®èµ„äº§ä¸‹é™è¶‹åŠ¿
        will_churn = np.random.choice([0, 1], p=[0.78, 0.22])  # 22%æµå¤±ç‡
        
        for week in range(n_weeks):
            week_date = start_date + timedelta(weeks=week)
            
            # èµ„äº§ç±»ç‰¹å¾ï¼ˆå¸¦è¶‹åŠ¿å’Œå™ªå£°ï¼‰
            if will_churn and week >= 8:  # æµå¤±å‰4å‘¨å¼€å§‹èµ„äº§ä¸‹é™
                asset_decay = 1 - (week - 7) * np.random.uniform(0.08, 0.15)
                asset_decay = max(asset_decay, 0.3)
            else:
                asset_decay = 1.0
            
            # æ ¸å¿ƒèµ„äº§æŒ‡æ ‡
            stock_market_value = customer['base_assets'] * asset_decay * np.random.uniform(0.9, 1.1)
            total_guarantee = stock_market_value * np.random.uniform(0.95, 1.05)
            liquid_assets = total_guarantee * np.random.uniform(0.8, 0.95)
            
            # äº¤æ˜“è¡Œä¸º
            if will_churn and week >= 8:
                trade_multiplier = 1 - (week - 7) * 0.2
            else:
                trade_multiplier = 1.0
            
            a_stock_volume = max(0, total_guarantee * np.random.uniform(0.05, 0.3) * trade_multiplier)
            commission = a_stock_volume * np.random.uniform(0.0003, 0.0005)
            
            # ç›ˆäº
            daily_pnl = stock_market_value * np.random.uniform(-0.03, 0.03)
            
            # èµ„é‡‘æµåŠ¨ï¼ˆæµå¤±å‰ä¼šå¤§é¢è½¬å‡ºï¼‰
            if will_churn and week >= 10:
                fund_flow = -total_guarantee * np.random.uniform(0.2, 0.6)
            else:
                fund_flow = np.random.uniform(-5, 5)
            
            # è¡Œä¸ºç‰¹å¾
            login_days = np.random.randint(0, 5) if will_churn and week >= 8 else np.random.randint(2, 6)
            last_login_days_ago = np.random.randint(0, 3) if login_days > 0 else np.random.randint(5, 15)
            
            weekly_records.append({
                'customer_id': customer['customer_id'],
                'week': week,
                'week_date': week_date,
                'stock_market_value': stock_market_value,
                'total_guarantee': total_guarantee,
                'liquid_assets': liquid_assets,
                'a_stock_volume': a_stock_volume,
                'commission': commission,
                'daily_pnl': daily_pnl,
                'fund_flow': fund_flow,
                'login_days': login_days,
                'last_login_days_ago': last_login_days_ago,
                'will_churn': will_churn
            })
    
    weekly_df = pd.DataFrame(weekly_records)
    
    # åˆå¹¶å®¢æˆ·ä¿¡æ¯
    df = weekly_df.merge(customer_df, on='customer_id', how='left')
    
    return df

# ==================== 2. è¯åˆ¸è¡Œä¸šç‰¹å¾å·¥ç¨‹ ====================
def feature_engineering_securities(df):
    """
    æ„å»ºè¯åˆ¸è¡Œä¸šæµå¤±é¢„è­¦ç‰¹å¾
    å®ç°å‘¨ç»Ÿè®¡é‡ã€å¤åˆå› å­ç­‰æ ¸å¿ƒç‰¹å¾
    """
    features = []
    
    for cust_id, group in df.groupby('customer_id'):
        group = group.sort_values('week')
        base_info = group.iloc[0]
        
        # è®¡ç®—å‘¨ç»Ÿè®¡é‡ï¼ˆå‡å€¼ã€æ ‡å‡†å·®ã€å˜å¼‚ç³»æ•°ã€æœ€å¤§å€¼ç­‰ï¼‰
        asset_features = {}
        
        # èµ„äº§ç±»å› å­ï¼ˆ5ä¸ªç»Ÿè®¡é‡ï¼‰
        for col in ['stock_market_value', 'total_guarantee', 'liquid_assets', 
                   'a_stock_volume', 'commission', 'daily_pnl']:
            values = group[col].values
            
            asset_features[f'{col}_mean'] = np.mean(values)
            asset_features[f'{col}_std'] = np.std(values)
            asset_features[f'{col}_cv'] = np.std(values) / (np.mean(values) + 1e-6)
            asset_features[f'{col}_max'] = np.max(values)
            asset_features[f'{col}_trend'] = np.polyfit(range(len(values)), values, 1)[0]
        
        # éèµ„äº§ç±»å› å­
        behavior_features = {
            'login_days_mean': np.mean(group['login_days']),
            'login_days_trend': np.polyfit(range(len(group)), group['login_days'], 1)[0],
            'last_login_max': np.max(group['last_login_days_ago']),
            'fund_flow_total': np.sum(group['fund_flow']),
            'fund_flow_negative_weeks': np.sum(group['fund_flow'] < -10),
            
            # åŸºç¡€ä¿¡æ¯
            'age': base_info['age'],
            'has_financial_product': base_info['has_financial_product'],
            'has_visited': base_info['has_visited'],
            
            # ç¼–ç ç±»ç‰¹å¾
            'customer_type': base_info['customer_type'],
            'risk_tolerance': base_info['risk_tolerance']
        }
        
        # å¤åˆå› å­
        composite_features = {
            'pnl_to_guarantee': asset_features['daily_pnl_mean'] / (asset_features['total_guarantee_mean'] + 1e-6),
            'guarantee_to_volume': asset_features['total_guarantee_mean'] / (asset_features['a_stock_volume_mean'] + 1e-6)
        }
        
        # åˆå¹¶æ‰€æœ‰ç‰¹å¾
        feature_dict = {
            'customer_id': cust_id,
            'will_churn': base_info['will_churn'],
            **asset_features,
            **behavior_features,
            **composite_features
        }
        
        features.append(feature_dict)
    
    feature_df = pd.DataFrame(features)
    
    # ç¼–ç ç±»åˆ«å˜é‡
    feature_df['customer_type'] = feature_df['customer_type'].map({
        'retail': 0, 'vip': 1, 'institution': 2
    })
    feature_df['risk_tolerance'] = feature_df['risk_tolerance'].map({
        'conservative': 0, 'moderate': 1, 'aggressive': 2
    })
    
    return feature_df

# ==================== 3. æ¨¡å‹è®­ç»ƒä¸è¯„ä¼° ====================
def train_securities_churn_model(df):
    """
    è®­ç»ƒè¯åˆ¸è¡Œä¸šæµå¤±é¢„è­¦æ¨¡å‹
    """
    # å‡†å¤‡ç‰¹å¾
    feature_cols = [col for col in df.columns if col not in ['customer_id', 'will_churn']]
    X = df[feature_cols]
    y = df['will_churn']
    
    print(f"ç‰¹å¾æ•°é‡: {len(feature_cols)}")
    print(f"æ ·æœ¬æ•°é‡: {len(df)}")
    print(f"æµå¤±ç‡: {y.mean():.2%}")
    
    # åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )
    
    # æ ‡å‡†åŒ–æ•°å€¼ç‰¹å¾
    scaler = StandardScaler()
    numeric_cols = X_train.select_dtypes(include=['float64', 'int64']).columns
    X_train[numeric_cols] = scaler.fit_transform(X_train[numeric_cols])
    X_test[numeric_cols] = scaler.transform(X_test[numeric_cols])
    
    # è®­ç»ƒXGBoost
    model = xgb.XGBClassifier(
        n_estimators=300,
        max_depth=5,
        learning_rate=0.05,
        subsample=0.8,
        colsample_bytree=0.8,
        scale_pos_weight=3.5,  # å¤„ç†æ ·æœ¬ä¸å¹³è¡¡
        random_state=42,
        eval_metric='auc',
        tree_method='hist'
    )
    
    model.fit(X_train, y_train)
    
    # é¢„æµ‹ä¸è¯„ä¼°
    y_pred = model.predict(X_test)
    y_proba = model.predict_proba(X_test)[:, 1]
    
    print("\n" + "="*60)
    print("è¯åˆ¸å®¢æˆ·æµå¤±é¢„è­¦æ¨¡å‹è¯„ä¼°")
    print("="*60)
    print(f"æµ‹è¯•é›†AUC: {roc_auc_score(y_test, y_proba):.4f}")
    print("\nåˆ†ç±»æŠ¥å‘Š:")
    print(classification_report(y_test, y_pred, 
                               target_names=['ç•™å­˜', 'æµå¤±']))
    
    return model, X_train, X_test, y_test, feature_cols, scaler

# ==================== 4. SHAPç‰¹å¾é‡è¦æ€§åˆ†æ ====================
def shap_analysis_securities(model, X_train, feature_cols):
    """
    è¯åˆ¸è¡Œä¸šSHAPåˆ†æ
    è¯†åˆ«å…³é”®æµå¤±é©±åŠ¨å› ç´ 
    """
    print("\nè¿›è¡ŒSHAPå¯è§£é‡Šæ€§åˆ†æ...")
    
    # é‡‡æ ·åŠ é€Ÿåˆ†æ
    X_sample = X_train.sample(n=1000, random_state=42)
    
    explainer = shap.TreeExplainer(model)
    shap_values = explainer.shap_values(X_sample)
    
    # å¯è§†åŒ–
    plt.figure(figsize=(15, 10))
    
    # ç‰¹å¾é‡è¦æ€§
    plt.subplot(2, 2, 1)
    shap.summary_plot(shap_values, X_sample, plot_type="bar", show=False)
    plt.title("è¯åˆ¸å®¢æˆ·æµå¤±å…³é”®ç‰¹å¾æ’å", fontsize=14)
    
    # åœ¨ç½‘æ—¶é•¿å½±å“
    plt.subplot(2, 2, 2)
    shap.dependence_plot("tenure", shap_values, X_sample, show=False)
    plt.title("åœ¨ç½‘æ—¶é•¿(tenure)å½±å“", fontsize=14)
    
    # èµ„äº§æ³¢åŠ¨å½±å“
    plt.subplot(2, 2, 3)
    shap.dependence_plot("stock_market_value_cv", shap_values, X_sample, show=False)
    plt.title("èµ„äº§æ³¢åŠ¨ç‡å½±å“", fontsize=14)
    
    # èµ„é‡‘æµå‡ºå½±å“
    plt.subplot(2, 2, 4)
    shap.dependence_plot("fund_flow_negative_weeks", shap_values, X_sample, show=False)
    plt.title("èµ„é‡‘å‡€æµå‡ºå‘¨æ•°å½±å“", fontsize=14)
    
    plt.tight_layout()
    plt.savefig('securities_shap_analysis.png', dpi=300, bbox_inches='tight')
    print("âœ… SHAPåˆ†æå›¾è¡¨å·²ä¿å­˜: securities_shap_analysis.png")
    plt.show()
    
    return explainer, shap_values

# ==================== 5. æ‰¹é‡é¢„æµ‹ä¸æŒ½ç•™ç­–ç•¥ ====================
def generate_securities_retention_plan(customer_info, risk_score):
    """
    ç”Ÿæˆè¯åˆ¸è¡Œä¸šä¸“å±æŒ½ç•™æ–¹æ¡ˆ
    """
    risk_level = "é«˜" if risk_score > 0.7 else "ä¸­" if risk_score > 0.4 else "ä½"
    
    strategies = {
        "é«˜": [
            "å®¢æˆ·ç»ç†48å°æ—¶å†…ç”µè¯å›è®¿ï¼Œæä¾›æŠ•èµ„ç»„åˆè¯Šæ–­",
            "èµ é€3ä¸ªæœˆLevel2è¡Œæƒ…+ä¸“å±æŠ•é¡¾æœåŠ¡",
            "æ ¹æ®æŒä»“æä¾›å®šåˆ¶åŒ–è°ƒä»“å»ºè®®"
        ],
        "ä¸­": [
            "æ¨é€è¿‘æœŸçƒ­é—¨ç ”æŠ¥å’Œå¸‚åœºåˆ†æ",
            "é‚€è¯·å‚åŠ çº¿ä¸‹æŠ•èµ„ç­–ç•¥ä¼š",
            "æä¾›ä½£é‡‘ä¼˜æƒ æ–¹æ¡ˆ"
        ],
        "ä½": [
            "çŸ­ä¿¡å…³æ€€æé†’å¸‚åœºæœºä¼š",
            "æ¨é€APPæ–°åŠŸèƒ½å¼•å¯¼",
            "ç§¯åˆ†å•†åŸä¼˜æƒ åˆ¸æ¿€åŠ±"
        ]
    }
    
    plan = {
        "å®¢æˆ·ID": customer_info['customer_id'],
        "é£é™©ç­‰çº§": risk_level,
        "æµå¤±æ¦‚ç‡": f"{risk_score:.1%}",
        "å…³é”®é¢„è­¦ä¿¡å·": f"èµ„äº§æ³¢åŠ¨ç‡: {customer_info.get('stock_market_value_cv', 0):.2f}, "
                       f"èµ„é‡‘å‡€æµå‡ºå‘¨æ•°: {customer_info.get('fund_flow_negative_weeks', 0)}",
        "æŒ½ç•™ç­–ç•¥": strategies[risk_level],
        "æ‰§è¡ŒæœŸé™": "3ä¸ªå·¥ä½œæ—¥" if risk_level == "é«˜" else "7ä¸ªå·¥ä½œæ—¥"
    }
    
    return plan

def batch_predict_and_generate_plan(model, df, feature_cols, scaler, top_k=30):
    """
    æ‰¹é‡é¢„æµ‹è¯åˆ¸é«˜æµå¤±é£é™©å®¢æˆ·å¹¶ç”ŸæˆæŒ½ç•™è®¡åˆ’
    """
    print("\n" + "="*60)
    print("å¼€å§‹æ‰¹é‡é¢„æµ‹ä¸æŒ½ç•™è®¡åˆ’ç”Ÿæˆ")
    print("="*60)
    
    # å‡†å¤‡ç‰¹å¾
    X = df[feature_cols]
    numeric_cols = X.select_dtypes(include=['float64', 'int64']).columns
    X[numeric_cols] = scaler.transform(X[numeric_cols])
    
    # é¢„æµ‹
    df['churn_prob'] = model.predict_proba(X)[:, 1]
    
    # è¯†åˆ«é«˜é£é™©å®¢æˆ·
    high_risk = df[df['churn_prob'] > 0.6].copy()
    high_risk = high_risk.sort_values('churn_prob', ascending=False).head(top_k)
    
    print(f"\nè¯†åˆ«å‡º {len(high_risk)} ä¸ªé«˜å±å®¢æˆ·ï¼ˆæµå¤±æ¦‚ç‡>60%ï¼‰")
    
    # ç”ŸæˆæŒ½ç•™è®¡åˆ’
    action_plans = []
    for _, customer in high_risk.iterrows():
        plan = generate_securities_retention_plan(customer.to_dict(), customer['churn_prob'])
        action_plans.append(plan)
        
        # æ‰“å°Top5è¯¦æƒ…
        if len(action_plans) <= 5:
            print(f"\nã€{plan['å®¢æˆ·ID']}ã€‘é£é™©ç­‰çº§: {plan['é£é™©ç­‰çº§']} | æµå¤±æ¦‚ç‡: {plan['æµå¤±æ¦‚ç‡']}")
            print(f"å…³é”®ä¿¡å·: {plan['å…³é”®é¢„è­¦ä¿¡å·']}")
            print("æŒ½ç•™ç­–ç•¥:")
            for idx, strategy in enumerate(plan['æŒ½ç•™ç­–ç•¥'], 1):
                print(f"  {idx}. {strategy}")
            print(f"æ‰§è¡ŒæœŸé™: {plan['æ‰§è¡ŒæœŸé™']}")
    
    # ä¿å­˜è®¡åˆ’
    action_df = pd.DataFrame(action_plans)
    action_df.to_excel('securities_retention_plan.xlsx', index=False)
    print(f"\nâœ… æŒ½ç•™è®¡åˆ’å·²ä¿å­˜è‡³: securities_retention_plan.xlsx")
    
    return high_risk, action_df

# ==================== 6. ä¸»æµç¨‹ ====================
def main():
    """è¯åˆ¸å®¢æˆ·æµå¤±é¢„è­¦ä¸»æµç¨‹"""
    print("ğŸš€ å¯åŠ¨è¯åˆ¸è¡Œä¸šå®¢æˆ·æµå¤±é¢„è­¦ç³»ç»Ÿ...")
    
    # 1. ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®
    print("\n1. ç”Ÿæˆè¯åˆ¸å®¢æˆ·å‘¨åº¦æ•°æ®...")
    weekly_df = generate_securities_data(n_customers=3000, n_weeks=12)
    print(f"   æ•°æ®è§„æ¨¡: {weekly_df.shape}")
    print(f"   å®¢æˆ·æ•°: {weekly_df['customer_id'].nunique()}")
    print(f"   æµå¤±ç‡: {weekly_df['will_churn'].mean():.2%}")
    
    # 2. ç‰¹å¾å·¥ç¨‹
    print("\n2. æ„å»ºè¯åˆ¸è¡Œä¸šç‰¹å¾...")
    feature_df = feature_engineering_securities(weekly_df)
    print(f"   ç‰¹å¾æ•°é‡: {feature_df.shape[1] - 2}")  # æ’é™¤idå’Œlabel
    
    # 3. è®­ç»ƒæ¨¡å‹
    print("\n3. è®­ç»ƒæµå¤±é¢„è­¦æ¨¡å‹...")
    model, X_train, X_test, y_test, feature_cols, scaler = train_securities_churn_model(feature_df)
    
    # 4. SHAPåˆ†æ
    print("\n4. è¿›è¡Œæ¨¡å‹å¯è§£é‡Šæ€§åˆ†æ...")
    explainer, shap_values = shap_analysis_securities(model, X_train, feature_cols)
    
    # 5. æ‰¹é‡é¢„æµ‹ä¸æŒ½ç•™è®¡åˆ’
    print("\n5. ç”Ÿæˆé«˜å±å®¢æˆ·æŒ½ç•™è®¡åˆ’...")
    high_risk_customers, action_df = batch_predict_and_generate_plan(
        model, feature_df, feature_cols, scaler, top_k=30
    )
    
    print("\nâœ… è¯åˆ¸å®¢æˆ·æµå¤±é¢„è­¦åˆ†æå®Œæˆï¼")
    print("   ç”Ÿæˆæ–‡ä»¶:")
    print("   - securities_shap_analysis.png (ç‰¹å¾é‡è¦æ€§)")
    print("   - securities_retention_plan.xlsx (æŒ½ç•™è®¡åˆ’)")

if __name__ == "__main__":
    main()

# ==================== 7. ä¾èµ–åŒ…è¯´æ˜ ====================
"""
requirements_securities.txt å†…å®¹:

# æ ¸å¿ƒæ•°æ®å¤„ç†
pandas>=2.0.0
numpy>=1.24.0

# æœºå™¨å­¦ä¹ 
scikit-learn>=1.3.0
xgboost>=2.0.0

# æ¨¡å‹è§£é‡Š
shap>=0.43.0

# å¯è§†åŒ–
matplotlib>=3.7.0
seaborn>=0.12.0

# Excelè¾“å‡º
openpyxl>=3.1.0

# å¯é€‰ï¼šæ¥å…¥çœŸå®LLMç”Ÿæˆç­–ç•¥
# dashscope>=1.19.0  # é˜¿é‡Œäº‘é€šä¹‰åƒé—®SDK

å®‰è£…å‘½ä»¤:
pip install -r requirements_securities.txt
"""
