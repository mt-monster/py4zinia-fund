import pandas as pd
import numpy as np
import matplotlib
matplotlib.use('Agg')  # è®¾ç½®éäº¤äº’å¼åç«¯ï¼Œè§£å†³Flaskå¤šçº¿ç¨‹é—®é¢˜
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, roc_auc_score
import xgboost as xgb
import shap
import json
import os
from openai import OpenAI

# è®¾ç½®matplotlibæ”¯æŒä¸­æ–‡æ˜¾ç¤º
plt.rcParams['font.sans-serif'] = ['SimHei']  # ä½¿ç”¨é»‘ä½“
plt.rcParams['axes.unicode_minus'] = False  # è§£å†³è´Ÿå·æ˜¾ç¤ºé—®é¢˜

# ==================== 1. æ•°æ®ç”Ÿæˆ ====================
def generate_securities_data(n_customers=3000, n_weeks=12):
    """
    ç”Ÿæˆè¯åˆ¸å®¢æˆ·å‘¨åº¦è¡Œä¸ºæ•°æ®
    """
    np.random.seed(42)
    
    # å®¢æˆ·åŸºç¡€ä¿¡æ¯
    customer_ids = [f'CUST_{i:06d}' for i in range(n_customers)]
    customer_types = np.random.choice(['retail', 'vip', 'institution'], n_customers, p=[0.7, 0.2, 0.1])
    risk_tolerances = np.random.choice(['conservative', 'moderate', 'aggressive'], n_customers, p=[0.4, 0.4, 0.2])
    base_assets = np.random.lognormal(10, 2, n_customers)
    base_assets[customer_types == 'institution'] = np.random.lognormal(14, 1.5, sum(customer_types == 'institution'))
    
    # ç”Ÿæˆå‘¨åº¦æ•°æ®
    weekly_data = []
    for i in range(n_weeks):
        week_data = {
            'customer_id': customer_ids,
            'customer_type': customer_types,
            'risk_tolerance': risk_tolerances,
            'week': i + 1,
            'base_assets': base_assets * (1 + np.random.normal(0, 0.05, n_customers)),
            'stock_market_value_mean': base_assets * np.random.uniform(0.3, 0.8, n_customers) * (1 + np.random.normal(0, 0.08, n_customers)),
            'stock_market_value_std': base_assets * np.random.uniform(0.05, 0.2, n_customers) * (1 + np.random.normal(0, 0.1, n_customers)),
            'stock_market_value_cv': np.random.uniform(0.1, 0.8, n_customers),
            'a_stock_volume_mean': np.random.lognormal(8, 2, n_customers),
            'a_stock_volume_std': np.random.lognormal(6, 2, n_customers),
            'a_stock_volume_cv': np.random.uniform(0.1, 0.5, n_customers),
            'transaction_frequency': np.random.poisson(3, n_customers),
            'commission_fee_mean': np.random.lognormal(4, 1, n_customers),
            'commission_fee_std': np.random.lognormal(2, 1, n_customers),
            'commission_cv': np.random.uniform(0.1, 0.6, n_customers),
            'daily_pnl_mean': np.random.normal(0, 1000, n_customers),
            'daily_pnl_std': np.random.uniform(500, 5000, n_customers),
            'daily_pnl_cv': np.random.uniform(0.2, 1.0, n_customers),
            'liquid_assets_mean': base_assets * np.random.uniform(0.2, 0.5, n_customers),
            'liquid_assets_std': base_assets * np.random.uniform(0.05, 0.15, n_customers),
            'liquid_assets_cv': np.random.uniform(0.1, 0.5, n_customers),
            'total_guarantee_mean': base_assets * np.random.uniform(0, 0.3, n_customers),
            'total_guarantee_std': base_assets * np.random.uniform(0, 0.1, n_customers),
            'total_guarantee_cv': np.random.uniform(0.1, 1.0, n_customers),
            'login_days_mean': np.random.poisson(15, n_customers),
            'login_days_std': np.random.uniform(1, 10, n_customers),
            'fund_flow_mean': np.random.normal(0, 10000, n_customers),
            'fund_flow_std': np.random.uniform(5000, 50000, n_customers),
            'fund_flow_cv': np.random.uniform(0.2, 1.0, n_customers),
            'fund_flow_negative_weeks': np.random.poisson(1, n_customers),
            'will_churn': 0  # åˆå§‹åŒ–æµå¤±æ ‡ç­¾ä¸º0
        }
        weekly_data.append(pd.DataFrame(week_data))
    
    # åˆå¹¶æ‰€æœ‰å‘¨æ•°æ®
    df = pd.concat(weekly_data, ignore_index=True)
    
    # ä¸º30%çš„å®¢æˆ·æ·»åŠ æµå¤±è¡Œä¸ºæ¨¡å¼
    churn_customers = np.random.choice(customer_ids, int(n_customers * 0.3), replace=False)
    
    for customer in churn_customers:
        customer_data = df[df['customer_id'] == customer]
        if len(customer_data) < 3:
            continue
            
        # åœ¨æœ€åå‡ å‘¨è®¾ç½®æµå¤±æ ‡ç­¾
        churn_week = np.random.randint(3, n_weeks)
        df.loc[(df['customer_id'] == customer) & (df['week'] >= churn_week), 'will_churn'] = 1
        
        # æ¨¡æ‹Ÿæµå¤±å‰çš„èµ„äº§å’Œç™»å½•è¡Œä¸ºå˜åŒ–
        df.loc[(df['customer_id'] == customer) & (df['week'] >= churn_week - 2), 'base_assets'] *= 0.8
        df.loc[(df['customer_id'] == customer) & (df['week'] >= churn_week - 2), 'login_days_mean'] *= 0.5
        df.loc[(df['customer_id'] == customer) & (df['week'] >= churn_week - 2), 'fund_flow_negative_weeks'] += 1
    
    # è®¡ç®—è‚¡ç¥¨å¸‚å€¼è¶‹åŠ¿ï¼ˆæœ€åå‡ å‘¨èµ„äº§ä¸‹é™æ¯”ä¾‹ï¼‰
    for customer in df['customer_id'].unique():
        customer_data = df[df['customer_id'] == customer].sort_values('week')
        if len(customer_data) >= 3:
            recent_assets = customer_data['base_assets'].tail(3).values
            if recent_assets[0] > 0:
                trend = (recent_assets[-1] - recent_assets[0]) / recent_assets[0]
                df.loc[df['customer_id'] == customer, 'stock_market_value_trend'] = trend
            else:
                df.loc[df['customer_id'] == customer, 'stock_market_value_trend'] = -1.0
        else:
            df.loc[df['customer_id'] == customer, 'stock_market_value_trend'] = 0.0
    
    return df

# ==================== 2. ç‰¹å¾å·¥ç¨‹ ====================
def feature_engineering_securities(df):
    """
    è¯åˆ¸è¡Œä¸šç‰¹å¾å·¥ç¨‹
    """
    # èšåˆå‘¨åº¦æ•°æ®åˆ°å®¢æˆ·çº§åˆ«
    customer_level = df.groupby('customer_id').agg({
        'customer_type': 'first',
        'risk_tolerance': 'first',
        'base_assets': 'mean',
        'stock_market_value_mean': 'mean',
        'stock_market_value_std': 'mean',
        'stock_market_value_cv': 'mean',
        'a_stock_volume_mean': 'mean',
        'a_stock_volume_std': 'mean',
        'a_stock_volume_cv': 'mean',
        'transaction_frequency': 'sum',
        'commission_fee_mean': 'mean',
        'commission_fee_std': 'mean',
        'commission_cv': 'mean',
        'daily_pnl_mean': 'mean',
        'daily_pnl_std': 'mean',
        'daily_pnl_cv': 'mean',
        'liquid_assets_mean': 'mean',
        'liquid_assets_std': 'mean',
        'liquid_assets_cv': 'mean',
        'total_guarantee_mean': 'mean',
        'total_guarantee_std': 'mean',
        'total_guarantee_cv': 'mean',
        'login_days_mean': 'mean',
        'login_days_std': 'mean',
        'fund_flow_mean': 'mean',
        'fund_flow_std': 'mean',
        'fund_flow_cv': 'mean',
        'fund_flow_negative_weeks': 'max',
        'stock_market_value_trend': 'mean',
        'will_churn': 'max'  # åªè¦æœ‰ä¸€å‘¨æ ‡è®°ä¸ºæµå¤±ï¼Œåˆ™æ ‡è®°ä¸ºæµå¤±å®¢æˆ·
    }).reset_index()
    
    feature_df = customer_level.copy()
    
    # ç¼–ç ç±»åˆ«å˜é‡
    feature_df['customer_type'] = feature_df['customer_type'].map({
        'retail': 0, 'vip': 1, 'institution': 2
    })
    feature_df['risk_tolerance'] = feature_df['risk_tolerance'].map({
        'conservative': 0, 'moderate': 1, 'aggressive': 2
    })
    
    return feature_df

# ==================== 3. æ¨¡å‹è®­ç»ƒä¸è¯„ä¼° ====================
def train_securities_churn_model(df):
    """
    è®­ç»ƒè¯åˆ¸è¡Œä¸šæµå¤±é¢„è­¦æ¨¡å‹
    """
    # å‡†å¤‡ç‰¹å¾
    feature_cols = [col for col in df.columns if col not in ['customer_id', 'will_churn']]
    X = df[feature_cols]
    y = df['will_churn']
    
    print(f"ç‰¹å¾æ•°é‡: {len(feature_cols)}")
    print(f"æ ·æœ¬æ•°é‡: {len(df)}")
    print(f"æµå¤±ç‡: {y.mean():.2%}")
    
    # åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )
    
    # æ ‡å‡†åŒ–æ•°å€¼ç‰¹å¾
    scaler = StandardScaler()
    numeric_cols = X_train.select_dtypes(include=['float64', 'int64']).columns
    X_train[numeric_cols] = scaler.fit_transform(X_train[numeric_cols])
    X_test[numeric_cols] = scaler.transform(X_test[numeric_cols])
    
    # è®­ç»ƒXGBoost
    model = xgb.XGBClassifier(
        n_estimators=300,
        max_depth=5,
        learning_rate=0.05,
        subsample=0.8,
        colsample_bytree=0.8,
        scale_pos_weight=3.5,  # å¤„ç†æ ·æœ¬ä¸å¹³è¡¡
        random_state=42,
        eval_metric='auc',
        tree_method='hist'
    )
    
    model.fit(X_train, y_train)
    
    # é¢„æµ‹ä¸è¯„ä¼°
    y_pred = model.predict(X_test)
    y_proba = model.predict_proba(X_test)[:, 1]
    
    print("\n" + "="*60)
    print("è¯åˆ¸å®¢æˆ·æµå¤±é¢„è­¦æ¨¡å‹è¯„ä¼°")
    print("="*60)
    print(f"æµ‹è¯•é›†AUC: {roc_auc_score(y_test, y_proba):.4f}")
    print("\nåˆ†ç±»æŠ¥å‘Š:")
    print(classification_report(y_test, y_pred, 
                               target_names=['ç•™å­˜', 'æµå¤±']))
    
    return model, X_train, X_test, y_test, feature_cols, scaler

# ==================== 4. SHAPç‰¹å¾é‡è¦æ€§åˆ†æ ====================
def shap_analysis_securities(model, X_train, feature_cols):
    """
    è¯åˆ¸è¡Œä¸šSHAPåˆ†æ
    è¯†åˆ«å…³é”®æµå¤±é©±åŠ¨å› ç´ 
    """
    print("\nè¿›è¡ŒSHAPå¯è§£é‡Šæ€§åˆ†æ...")
    
    # é‡‡æ ·åŠ é€Ÿåˆ†æ
    X_sample = X_train.sample(n=1000, random_state=42)
    
    explainer = shap.TreeExplainer(model)
    shap_values = explainer.shap_values(X_sample)
    
    # ä¸ºç‰¹å¾åˆ›å»ºä¸­æ–‡æ˜ å°„å­—å…¸
    feature_name_mapping = {
        'stock_market_value_cv': 'è‚¡ç¥¨å¸‚å€¼æ³¢åŠ¨ç‡',
        'fund_flow_negative_weeks': 'èµ„é‡‘å‡€æµå‡ºå‘¨æ•°',
        'login_days_mean': 'å¹³å‡ç™»å½•å¤©æ•°',
        'stock_market_value_trend': 'è‚¡ç¥¨å¸‚å€¼è¶‹åŠ¿',
        'a_stock_volume_cv': 'Aè‚¡äº¤æ˜“é‡æ³¢åŠ¨ç‡',
        'commission_cv': 'ä½£é‡‘æ³¢åŠ¨ç‡',
        'daily_pnl_cv': 'æ—¥ç›ˆäºæ³¢åŠ¨ç‡',
        'liquid_assets_cv': 'æµåŠ¨èµ„äº§æ³¢åŠ¨ç‡',
        'total_guarantee_cv': 'æ€»æ‹…ä¿é‡‘æ³¢åŠ¨ç‡',
        'liquid_assets_trend': 'æµåŠ¨èµ„äº§è¶‹åŠ¿',
        'risk_tolerance': 'é£é™©æ‰¿å—èƒ½åŠ›',
        'customer_type': 'å®¢æˆ·ç±»å‹',
        'guarantee_to_volume': 'æ‹…ä¿é‡‘å‘¨è½¬ç‡',
        'pnl_to_guarantee': 'ç›ˆäºæ‹…ä¿æ¯”',
        'last_login_max': 'æœ€åç™»å½•é—´éš”',
        'fund_flow_total': 'èµ„é‡‘æµæ€»é¢',
        'age': 'å¹´é¾„',
        'has_financial_product': 'æ˜¯å¦æœ‰é‡‘èäº§å“',
        'has_visited': 'æ˜¯å¦è®¿é—®è¿‡',
        'daily_pnl_trend': 'æ—¥ç›ˆäºè¶‹åŠ¿',
        'login_days_trend': 'ç™»å½•å¤©æ•°è¶‹åŠ¿',
        'commission_trend': 'ä½£é‡‘è¶‹åŠ¿',
        'commission_max': 'æœ€å¤§ä½£é‡‘'
    }
    
    # åˆ›å»ºä¸­æ–‡ç‰¹å¾ååˆ—è¡¨
    chinese_feature_names = [feature_name_mapping.get(col, col) for col in feature_cols]
    
    # ä¿å­˜è·¯å¾„è®¾ç½®
    save_path = 'D:/codes/py4zinia/customer_churn_predict/'
    
    # 1. ç‰¹å¾é‡è¦æ€§æŸ±çŠ¶å›¾
    plt.figure(figsize=(12, 8))
    shap.summary_plot(shap_values, X_sample, plot_type="bar", show=False, 
                     feature_names=chinese_feature_names)
    plt.title("è¯åˆ¸å®¢æˆ·æµå¤±å…³é”®ç‰¹å¾æ’å", fontsize=14)
    plt.tight_layout()
    plt.savefig(f'{save_path}securities_shap_bar.png', dpi=300, bbox_inches='tight')
    print(f"âœ… SHAPç‰¹å¾é‡è¦æ€§æŸ±çŠ¶å›¾å·²ä¿å­˜: {save_path}securities_shap_bar.png")
    plt.close()
    
    # 2. SHAPå€¼æ•£ç‚¹å›¾
    plt.figure(figsize=(12, 8))
    shap.summary_plot(shap_values, X_sample, show=False, 
                     feature_names=chinese_feature_names)
    plt.title("è¯åˆ¸å®¢æˆ·æµå¤±ç‰¹å¾SHAPå€¼æ•£ç‚¹å›¾", fontsize=14)
    plt.tight_layout()
    plt.savefig(f'{save_path}securities_shap_scatter.png', dpi=300, bbox_inches='tight')
    print(f"âœ… SHAPç‰¹å¾é‡è¦æ€§æ•£ç‚¹å›¾å·²ä¿å­˜: {save_path}securities_shap_scatter.png")
    plt.close()
    
    # 3. ç»¼åˆä¾èµ–å›¾
    plt.figure(figsize=(15, 12))
    
    # é€‰æ‹©å‰4ä¸ªæœ€é‡è¦çš„ç‰¹å¾è¿›è¡Œå¯è§†åŒ–ï¼ˆä½¿ç”¨ç´¢å¼•ï¼‰
    for i in range(4):
        plt.subplot(2, 2, i+1)
        shap.dependence_plot(i, shap_values, X_sample, 
                            feature_names=chinese_feature_names,
                            show=False)
        plt.title(f"{chinese_feature_names[i]}å½±å“", fontsize=14)
    
    plt.tight_layout()
    plt.savefig(f'{save_path}securities_shap_dependence.png', dpi=300, bbox_inches='tight')
    print(f"âœ… SHAPç‰¹å¾ä¾èµ–å›¾å·²ä¿å­˜: {save_path}securities_shap_dependence.png")
    plt.close()
    
    return explainer, shap_values

# ==================== 5. å¤§æ¨¡å‹ç”ŸæˆæŒ½ç•™ç­–ç•¥ ====================
def generate_retention_strategy(customer_profile, risk_score, shap_contrib):
    """
    è°ƒç”¨å­—èŠ‚è·³åŠ¨æ–¹èˆŸå¤§æ¨¡å‹ç”Ÿæˆä¸ªæ€§åŒ–æŒ½ç•™ç­–ç•¥
    
    å¦‚æœæ²¡æœ‰APIå¯†é’¥æˆ–è°ƒç”¨å¤±è´¥ï¼Œä¼šè¿”å›æ¨¡æ‹Ÿçš„ç­–ç•¥æ–‡æœ¬
    """
    prompt = f"""
    ä½ æ˜¯ä¸€ä½ç»éªŒä¸°å¯Œçš„è¯åˆ¸å®¢æˆ·å…³ç³»ç®¡ç†ä¸“å®¶ã€‚è¯·æ ¹æ®ä»¥ä¸‹å®¢æˆ·ä¿¡æ¯ï¼Œç”Ÿæˆ3æ¡å…·ä½“ã€å¯æ‰§è¡Œçš„æŒ½ç•™ç­–ç•¥ã€‚
    
    å®¢æˆ·æƒ…å†µï¼š
    - æµå¤±é£é™©ï¼š{risk_score:.1%}
    - å®¢æˆ·ç±»å‹ï¼š{'é›¶å”®å®¢æˆ·' if customer_profile.get('customer_type') == 'retail' else 'VIPå®¢æˆ·' if customer_profile.get('customer_type') == 'vip' else 'æœºæ„å®¢æˆ·'}
    - æ€»èµ„äº§ï¼šÂ¥{customer_profile.get('base_assets', 0):.2f}
    - ä¸»è¦é£é™©å› ç´ ï¼š{', '.join([f"{k}({v:.2f})" for k,v in list(shap_contrib.items())[:3]])}
    
    è¦æ±‚ï¼š
    1. ç­–ç•¥å¿…é¡»å…·ä½“æ˜ç¡®ï¼ŒåŒ…å«ä¼˜æƒ åŠ›åº¦å’Œæ²Ÿé€šè¯æœ¯
    2. è€ƒè™‘å®¢æˆ·ç”Ÿå‘½å‘¨æœŸä»·å€¼
    3. åŒºåˆ†é«˜/ä¸­/ä½ä¸åŒé£é™©ç­‰çº§
    4. è¾“å‡ºæ ¼å¼ï¼šJSONæ ¼å¼ï¼ŒåŒ…å«"é£é™©ç­‰çº§"ã€"ç­–ç•¥"ã€"é¢„æœŸæ•ˆæœ"ã€"æ‰§è¡Œä¼˜å…ˆçº§"
    """
    
    # è°ƒç”¨å­—èŠ‚è·³åŠ¨æ–¹èˆŸå¤§æ¨¡å‹
    try:
        # è¯·ç¡®ä¿æ‚¨å·²å°† API Key å­˜å‚¨åœ¨ç¯å¢ƒå˜é‡ ARK_API_KEY ä¸­
        # åˆå§‹åŒ–Openaiå®¢æˆ·ç«¯ï¼Œä»ç¯å¢ƒå˜é‡ä¸­è¯»å–æ‚¨çš„API Key
        client = OpenAI(
            # æ­¤ä¸ºé»˜è®¤è·¯å¾„ï¼Œæ‚¨å¯æ ¹æ®ä¸šåŠ¡æ‰€åœ¨åœ°åŸŸè¿›è¡Œé…ç½®
            base_url="https://ark.cn-beijing.volces.com/api/v3",
            api_key="7301bcd7-0207-4bc1-8bd7-8f64182fa1bb",
        )
        
        # Non-streaming request
        response = client.chat.completions.create(
            # æŒ‡å®šæ‚¨åˆ›å»ºçš„æ–¹èˆŸæ¨ç†æ¥å…¥ç‚¹ ID
            model="kimi-k2-thinking-251104",
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä½ç»éªŒä¸°å¯Œçš„è¯åˆ¸å®¢æˆ·å…³ç³»ç®¡ç†ä¸“å®¶"},
                {"role": "user", "content": prompt},
            ],
        )
        
        # è·å–å“åº”å†…å®¹
        content = response.choices[0].message.content
        
        # æ‰“å°å“åº”å†…å®¹ä»¥ä¾¿è°ƒè¯•
        print(f"APIå“åº”å†…å®¹: {content[:200]}...")
        
        # å¦‚æœå“åº”åŒ…å«JSONæ ¼å¼ï¼Œæå–çº¯JSONéƒ¨åˆ†
        if '{' in content:
            # æ‰¾åˆ°ç¬¬ä¸€ä¸ª'{'å’Œæœ€åä¸€ä¸ª'}'çš„ä½ç½®
            start_idx = content.find('{')
            end_idx = content.rfind('}') + 1
            if start_idx >= 0 and end_idx > start_idx:
                content = content[start_idx:end_idx]
        
        return content
    except Exception as e:
        print(f"å­—èŠ‚è·³åŠ¨æ–¹èˆŸAPIè°ƒç”¨å¼‚å¸¸ï¼š{str(e)}")
        # è¿”å›æ¨¡æ‹Ÿçš„ç­–ç•¥JSON
        return json.dumps({
            "é£é™©ç­‰çº§": "é«˜" if risk_score > 0.7 else "ä¸­" if risk_score > 0.4 else "ä½",
            "ç­–ç•¥": ["å®¢æˆ·ç»ç†ç”µè¯å›è®¿", "æä¾›ä¸ªæ€§åŒ–æŠ•èµ„å»ºè®®", "èµ é€æœåŠ¡ä¼˜æƒ åˆ¸"],
            "é¢„æœŸæ•ˆæœ": "é™ä½å®¢æˆ·æµå¤±é£é™©",
            "æ‰§è¡Œä¼˜å…ˆçº§": 1 if risk_score > 0.7 else 2 if risk_score > 0.4 else 3
        })

def batch_predict_and_generate_plan(model, df, feature_cols, scaler, explainer=None, top_k=30):
    """
    æ‰¹é‡é¢„æµ‹è¯åˆ¸é«˜æµå¤±é£é™©å®¢æˆ·å¹¶ç”ŸæˆæŒ½ç•™è®¡åˆ’
    
    å‚æ•°:
    - model: è®­ç»ƒå¥½çš„æ¨¡å‹
    - df: åŒ…å«å®¢æˆ·ç‰¹å¾çš„æ•°æ®æ¡†
    - feature_cols: ç‰¹å¾åˆ—ååˆ—è¡¨
    - scaler: ç”¨äºç‰¹å¾æ ‡å‡†åŒ–çš„ç¼©æ”¾å™¨
    - explainer: SHAPè§£é‡Šå™¨å¯¹è±¡ï¼Œç”¨äºè®¡ç®—ç‰¹å¾é‡è¦æ€§
    - top_k: ç”Ÿæˆè®¡åˆ’çš„å‰kä¸ªé«˜é£é™©å®¢æˆ·
    
    è¿”å›:
    - high_risk: é«˜é£é™©å®¢æˆ·æ•°æ®æ¡†
    - action_df: æŒ½ç•™è®¡åˆ’æ•°æ®æ¡†
    """
    print("\n" + "="*60)
    print("å¼€å§‹æ‰¹é‡é¢„æµ‹ä¸æŒ½ç•™è®¡åˆ’ç”Ÿæˆ")
    print("="*60)
    
    # å‡†å¤‡ç‰¹å¾
    X = df[feature_cols].copy()
    numeric_cols = X.select_dtypes(include=['float64', 'int64']).columns
    X[numeric_cols] = scaler.transform(X[numeric_cols])
    
    # é¢„æµ‹
    df['churn_prob'] = model.predict_proba(X)[:, 1]
    
    # è¯†åˆ«é«˜é£é™©å®¢æˆ·
    high_risk = df[df['churn_prob'] > 0.6].copy()
    high_risk = high_risk.sort_values('churn_prob', ascending=False).head(top_k)
    
    print(f"\nè¯†åˆ«å‡º {len(high_risk)} ä¸ªé«˜å±å®¢æˆ·ï¼ˆæµå¤±æ¦‚ç‡>60%ï¼‰")
    
    # ç”ŸæˆæŒ½ç•™è®¡åˆ’
    action_plans = []
    
    # è®¡ç®—SHAPå€¼ï¼ˆå¦‚æœæä¾›äº†è§£é‡Šå™¨ï¼‰
    shap_values = None
    if explainer is not None:
        shap_values = explainer.shap_values(X)
    
    for idx, customer in high_risk[:2].iterrows():
        # æå–å®¢æˆ·ä¿¡æ¯
        customer_info = customer.to_dict()
        risk_score = customer['churn_prob']
        
        # æå–SHAPè´¡çŒ®ï¼ˆå¦‚æœæœ‰ï¼‰
        shap_contrib = {}
        if shap_values is not None:
            # æ‰¾åˆ°å®¢æˆ·åœ¨åŸå§‹æ•°æ®ä¸­çš„ç´¢å¼•
            customer_idx = df.index.get_loc(idx)
            # æå–è¯¥å®¢æˆ·çš„SHAPå€¼
            customer_shap = shap_values[customer_idx]
            # æ„å»ºç‰¹å¾ä¸SHAPå€¼çš„æ˜ å°„
            shap_contrib = {col: customer_shap[i] for i, col in enumerate(feature_cols)}
        
        # è°ƒç”¨å¤§æ¨¡å‹ç”ŸæˆæŒ½ç•™ç­–ç•¥
        strategy_response = generate_retention_strategy(customer_info, risk_score, shap_contrib)
        
        # è§£æç­–ç•¥JSON
        strategy_data = {}
        
        try:
            # å¤åˆ¶å“åº”å†…å®¹ä»¥ä¾¿å¤„ç†
            response_content = strategy_response
            
            # æ¸…ç†markdownä»£ç å—æ ‡è®°
            if response_content.strip().startswith('```json'):
                response_content = response_content[response_content.find('\n')+1:]
            if response_content.strip().endswith('```'):
                response_content = response_content[:response_content.rfind('```')]
            response_content = response_content.strip()
            
            # å°è¯•è§£ææ•´ä¸ªå†…å®¹
            try:
                strategy_data = json.loads(response_content)
            except json.JSONDecodeError:
                # å¦‚æœæ•´ä¸ªå†…å®¹è§£æå¤±è´¥ï¼Œå°è¯•æå–JSONéƒ¨åˆ†
                print("å°è¯•æå–JSONéƒ¨åˆ†...")
                # æ‰¾åˆ°æ‰€æœ‰å¯èƒ½çš„JSONå¼€å§‹ä½ç½®
                json_starts = [i for i, char in enumerate(response_content) if char == '{']
                json_ends = [i for i, char in enumerate(response_content) if char == '}']
                
                # æ‰¾åˆ°æœ€é•¿çš„æœ‰æ•ˆJSON
                for start in reversed(json_starts):
                    for end in json_ends:
                        if end > start:
                            try:
                                json_part = response_content[start:end+1]
                                strategy_data = json.loads(json_part)
                                print("å·²æå–æœ‰æ•ˆçš„JSONéƒ¨åˆ†")
                                break
                            except json.JSONDecodeError:
                                continue
                    if strategy_data:
                        break
        except Exception as e:
            # å¦‚æœè§£æå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤ç­–ç•¥
            print(f"è­¦å‘Šï¼šç­–ç•¥JSONè§£æå¤±è´¥ - {str(e)}ï¼Œå·²ä½¿ç”¨é»˜è®¤ç­–ç•¥")
            strategy_data = {}
        
        
        # å¦‚æœstrategy_dataä¸ºç©ºæˆ–ç¼ºå°‘å…³é”®ä¿¡æ¯ï¼Œä½¿ç”¨é»˜è®¤å€¼
        if not strategy_data:
            risk_level = "é«˜" if risk_score > 0.7 else "ä¸­" if risk_score > 0.4 else "ä½"
            strategy_data = {
                "é£é™©ç­‰çº§": risk_level,
                "ç­–ç•¥": ["å®¢æˆ·ç»ç†ç”µè¯å›è®¿", "æä¾›ä¸ªæ€§åŒ–æŠ•èµ„å»ºè®®", "èµ é€æœåŠ¡ä¼˜æƒ åˆ¸"],
                "é¢„æœŸæ•ˆæœ": "é™ä½å®¢æˆ·æµå¤±é£é™©",
                "æ‰§è¡Œä¼˜å…ˆçº§": 1 if risk_score > 0.7 else 2 if risk_score > 0.4 else 3
            }
        
        # æ„å»ºå®Œæ•´çš„æŒ½ç•™è®¡åˆ’
        # æ ¹æ®risk_scoreè®¡ç®—æ­£ç¡®çš„é£é™©ç­‰çº§
        calculated_risk_level = "é«˜" if risk_score > 0.7 else "ä¸­" if risk_score > 0.4 else "ä½"
        
        # å¦‚æœå¤§æ¨¡å‹è¿”å›äº†é£é™©ç­‰çº§ï¼Œåˆ™ä½¿ç”¨å¤§æ¨¡å‹çš„ç»“æœï¼Œå¦åˆ™ä½¿ç”¨è®¡ç®—çš„é£é™©ç­‰çº§
        if "é£é™©ç­‰çº§" not in strategy_data:
            strategy_data["é£é™©ç­‰çº§"] = calculated_risk_level
        
        # æ›´æ–°æ‰§è¡Œä¼˜å…ˆçº§ï¼Œç¡®ä¿ä¸é£é™©ç­‰çº§åŒ¹é…
        if "æ‰§è¡Œä¼˜å…ˆçº§" not in strategy_data: 
            strategy_data["æ‰§è¡Œä¼˜å…ˆçº§"] = 1 if calculated_risk_level == "é«˜" else 2 if calculated_risk_level == "ä¸­" else 3
        
        # å¤„ç†æŒ½ç•™ç­–ç•¥ï¼ˆé€‚é…å¤§æ¨¡å‹è¿”å›çš„å­—å…¸æ ¼å¼ï¼‰
        if "ç­–ç•¥" in strategy_data:
            # å¦‚æœæœ‰"ç­–ç•¥"å­—æ®µï¼Œä½¿ç”¨è¯¥å­—æ®µ
            strategy = strategy_data["ç­–ç•¥"]
            if isinstance(strategy, dict):
                # å¦‚æœæ˜¯å­—å…¸æ ¼å¼ï¼Œè½¬æ¢ä¸ºåˆ—è¡¨ä»¥ä¾¿å±•ç¤º
                strategy_list = [f"{k}: {v}" for k, v in strategy.items()]
            else:
                strategy_list = strategy
        else:
            # å¦åˆ™ï¼Œå°†æ•´ä¸ªstrategy_dataä½œä¸ºç­–ç•¥å†…å®¹ï¼Œä½†æ’é™¤éç­–ç•¥å­—æ®µ
            strategy_list = []
            for k, v in strategy_data.items():
                if k not in ["é£é™©ç­‰çº§", "æ‰§è¡Œä¼˜å…ˆçº§", "é¢„æœŸæ•ˆæœ", "å¤‡æ³¨"]:
                    if isinstance(v, list):
                        # å¦‚æœæ˜¯åˆ—è¡¨ï¼Œè½¬æ¢ä¸ºå­—ç¬¦ä¸²
                        items = []
                        for item in v:
                            if isinstance(item, dict):
                                # å¦‚æœåˆ—è¡¨é¡¹æ˜¯å­—å…¸ï¼Œè½¬æ¢ä¸ºé”®å€¼å¯¹å­—ç¬¦ä¸²
                                item_str = "; ".join([f"{ik}: {iv}" for ik, iv in item.items()])
                                items.append(item_str)
                            elif isinstance(item, str):
                                # å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œç›´æ¥ä½¿ç”¨
                                items.append(item)
                            else:
                                # å…¶ä»–ç±»å‹ï¼Œè½¬æ¢ä¸ºå­—ç¬¦ä¸²
                                items.append(str(item))
                        v_str = ", ".join(items)
                        strategy_list.append(f"{k}: {v_str}")
                    else:
                        strategy_list.append(f"{k}: {v}")
        
        # å¤„ç†é¢„æœŸæ•ˆæœï¼ˆé€‚é…å¤§æ¨¡å‹è¿”å›çš„å­—å…¸æ ¼å¼ï¼‰
        expected_effect = strategy_data.get("é¢„æœŸæ•ˆæœ", "é™ä½å®¢æˆ·æµå¤±é£é™©")
        if isinstance(expected_effect, dict):
            # å¦‚æœæ˜¯å­—å…¸æ ¼å¼ï¼Œè½¬æ¢ä¸ºå­—ç¬¦ä¸²ä»¥ä¾¿å±•ç¤º
            expected_effect_str = ", ".join([f"{k}: {v}" for k, v in expected_effect.items()])
        else:
            expected_effect_str = expected_effect
        
        # è·å–æœ€ç»ˆçš„é£é™©ç­‰çº§ï¼ˆä¼˜å…ˆä½¿ç”¨å¤§æ¨¡å‹è¿”å›çš„ï¼Œå¦åˆ™ä½¿ç”¨è®¡ç®—çš„ï¼‰
        final_risk_level = strategy_data["é£é™©ç­‰çº§"]
        
        # æ„å»ºæŒ½ç•™è®¡åˆ’
        plan = {
            "å®¢æˆ·ID": customer_info['customer_id'],
            "é£é™©ç­‰çº§": final_risk_level,
            "æµå¤±æ¦‚ç‡": f"{risk_score:.1%}",
            "å…³é”®é¢„è­¦ä¿¡å·": f"èµ„äº§æ³¢åŠ¨ç‡: {customer_info.get('stock_market_value_cv', 0):.2f}, "
                           f"èµ„é‡‘å‡€æµå‡ºå‘¨æ•°: {customer_info.get('fund_flow_negative_weeks', 0)}",
            "æŒ½ç•™ç­–ç•¥": strategy_list,
            "é¢„æœŸæ•ˆæœ": expected_effect_str,
            "æ‰§è¡Œä¼˜å…ˆçº§": strategy_data.get("æ‰§è¡Œä¼˜å…ˆçº§", 1 if calculated_risk_level == "é«˜" else 2 if calculated_risk_level == "ä¸­" else 3),
            "æ‰§è¡ŒæœŸé™": "3ä¸ªå·¥ä½œæ—¥" if calculated_risk_level == "é«˜" else "7ä¸ªå·¥ä½œæ—¥",
            "å¤‡æ³¨": strategy_data.get("å¤‡æ³¨", "")
        }
        
        action_plans.append(plan)
        
        # æ‰“å°Top5è¯¦æƒ…
        if len(action_plans) <= 1:
            print(f"\nã€{plan['å®¢æˆ·ID']}ã€‘é£é™©ç­‰çº§: {plan['é£é™©ç­‰çº§']} | æµå¤±æ¦‚ç‡: {plan['æµå¤±æ¦‚ç‡']}")
            print(f"å…³é”®ä¿¡å·: {plan['å…³é”®é¢„è­¦ä¿¡å·']}")
            print("æŒ½ç•™ç­–ç•¥:")
            for idx, strategy in enumerate(plan['æŒ½ç•™ç­–ç•¥'], 1):
                print(f"  {idx}. {strategy}")
            print(f"é¢„æœŸæ•ˆæœ: {plan['é¢„æœŸæ•ˆæœ']}")
            print(f"æ‰§è¡Œä¼˜å…ˆçº§: {plan['æ‰§è¡Œä¼˜å…ˆçº§']}")
            print(f"æ‰§è¡ŒæœŸé™: {plan['æ‰§è¡ŒæœŸé™']}")
            if plan['å¤‡æ³¨']:
                print(f"å¤‡æ³¨: {plan['å¤‡æ³¨']}")
    
    # ä¿å­˜è®¡åˆ’
    action_df = pd.DataFrame(action_plans)
    action_df.to_excel('securities_retention_plan.xlsx', index=False)
    print(f"\nâœ… æŒ½ç•™è®¡åˆ’å·²ä¿å­˜è‡³: securities_retention_plan.xlsx")
    
    return high_risk, action_df

# ==================== 6. ä¸»æµç¨‹ ====================
def main():
    """è¯åˆ¸å®¢æˆ·æµå¤±é¢„è­¦ä¸»æµç¨‹"""
    print("ğŸš€ å¯åŠ¨è¯åˆ¸è¡Œä¸šå®¢æˆ·æµå¤±é¢„è­¦ç³»ç»Ÿ...")
    
    # 1. ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®
    print("\n1. ç”Ÿæˆè¯åˆ¸å®¢æˆ·å‘¨åº¦æ•°æ®...")
    weekly_df = generate_securities_data(n_customers=3000, n_weeks=12)
    print(f"   æ•°æ®è§„æ¨¡: {weekly_df.shape}")
    print(f"   å®¢æˆ·æ•°: {weekly_df['customer_id'].nunique()}")
    print(f"   æµå¤±ç‡: {weekly_df['will_churn'].mean():.2%}")
    
    # 2. ç‰¹å¾å·¥ç¨‹
    print("\n2. æ„å»ºè¯åˆ¸è¡Œä¸šç‰¹å¾...")
    feature_df = feature_engineering_securities(weekly_df)
    print(f"   ç‰¹å¾æ•°é‡: {feature_df.shape[1] - 2}")  # æ’é™¤idå’Œlabel
    
    # 3. è®­ç»ƒæ¨¡å‹
    print("\n3. è®­ç»ƒæµå¤±é¢„è­¦æ¨¡å‹...")
    model, X_train, X_test, y_test, feature_cols, scaler = train_securities_churn_model(feature_df)
    
    # 4. SHAPåˆ†æ
    print("\n4. è¿›è¡Œæ¨¡å‹å¯è§£é‡Šæ€§åˆ†æ...")
    explainer, shap_values = shap_analysis_securities(model, X_train, feature_cols)
    
    # 5. æ‰¹é‡é¢„æµ‹ä¸æŒ½ç•™è®¡åˆ’
    print("\n5. ç”Ÿæˆé«˜å±å®¢æˆ·æŒ½ç•™è®¡åˆ’...")
    high_risk_customers, action_df = batch_predict_and_generate_plan(
        model, feature_df, feature_cols, scaler, explainer=explainer, top_k=30
    )
    
    print("\nâœ… è¯åˆ¸å®¢æˆ·æµå¤±é¢„è­¦åˆ†æå®Œæˆï¼")
    print("   ç”Ÿæˆæ–‡ä»¶:")
    print("   - securities_shap_bar.png (ç‰¹å¾é‡è¦æ€§æŸ±çŠ¶å›¾)")
    print("   - securities_shap_scatter.png (ç‰¹å¾é‡è¦æ€§æ•£ç‚¹å›¾)")
    print("   - securities_shap_dependence.png (ç‰¹å¾ä¾èµ–å›¾)")
    print("   - securities_retention_plan.xlsx (æŒ½ç•™è®¡åˆ’)")

if __name__ == "__main__":
    main()